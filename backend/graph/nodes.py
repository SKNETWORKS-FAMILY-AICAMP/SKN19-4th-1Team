# backend/graph/nodes.py
"""
LangGraph ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ëŠ” ë…¸ë“œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.

ReAct íŒ¨í„´: LLMì´ ììœ¨ì ìœ¼ë¡œ tool í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì • (agent_node, should_continue)
"""

from langchain_core.messages import SystemMessage

from .state import MentorState
from backend.rag.retriever import (
    search_major_docs,
    aggregate_major_scores,
)
from backend.rag.embeddings import get_embeddings

from backend.rag.tools import (
    list_departments,
    get_universities_by_department,
    get_major_career_info,
    get_search_help,
    get_university_admission_info,
)

from backend.config import get_llm

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (.envì—ì„œ ì„¤ì •í•œ LLM_PROVIDERì™€ MODEL_NAME ì‚¬ìš©)
llm = get_llm()

# doc_typeë³„ ê¸°ë³¸ ê°€ì¤‘ì¹˜
MAJOR_DOC_WEIGHTS = {
    "summary": 0.8,
    "interest": 1.3,
    "property": 0.8,
    "subjects": 1.2,
    "jobs": 1.0,
}


# ==================== ReAct ì—ì´ì „íŠ¸ìš© ì„¤ì • ====================
# ReAct íŒ¨í„´: LLMì´ í•„ìš”ì‹œ ììœ¨ì ìœ¼ë¡œ íˆ´ì„ í˜¸ì¶œí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
tools = [
    list_departments,
    get_universities_by_department,
    get_major_career_info,
    get_search_help,
    get_university_admission_info,
]  # ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ ëª©ë¡
llm_with_tools = llm.bind_tools(tools)  # LLMì— íˆ´ ì‚¬ìš© ê¶Œí•œ ë¶€ì—¬


def _format_profile_value(value) -> str:
    # ì˜¨ë³´ë”© ë‹µë³€ì´ ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë“± ë‹¤ì–‘í•œ í˜•íƒœì—¬ì„œ ë¬¸ìì—´ë¡œ ê· ì¼í•˜ê²Œ ë³€í™˜
    if value is None:
        return ""
    if isinstance(value, str):
        return value.strip()
    if isinstance(value, (list, tuple, set)):
        items = [str(item).strip() for item in value if str(item).strip()]
        return ", ".join(items)
    if isinstance(value, dict):
        parts = []
        for key, sub_value in value.items():
            sub_text = _format_profile_value(sub_value)
            if sub_text:
                parts.append(f"{key}: {sub_text}")
        return "; ".join(parts)
    return str(value)


def _build_user_profile_text(answers: dict, fallback_question: str | None) -> str:
    # í•™ìƒì˜ ì„ í˜¸ ì •ë³´ë¥¼ í•œ ë©ì–´ë¦¬ í…ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ ì„ë² ë”©ì— í™œìš©
    if not answers and not fallback_question:
        return ""

    ordered_keys = [
        # ("preferred_majors", "ê´€ì‹¬ ì „ê³µ"), # [2025-12-12] ì§ˆë¬¸ ì œê±°ë¨
        ("subjects", "ì¢‹ì•„í•˜ëŠ” ê³¼ëª©"),
        ("interests", "ê´€ì‹¬ì‚¬/ì·¨ë¯¸"),
        ("activities", "êµë‚´/ëŒ€ì™¸ í™œë™"),
        ("desired_salary", "í¬ë§ ì—°ë´‰"),
        ("career_goal", "ì§„ë¡œ ëª©í‘œ"),
        ("strengths", "ê°•ì "),
        ("career_field", "ì¤‘ìš” ê°€ì¹˜ê´€"),
        ("topics", "ê´€ì‹¬ ì£¼ì œ"),
        ("learning_style", "í•™ìŠµ ìŠ¤íƒ€ì¼"),
    ]

    sections: list[str] = []
    used_keys = {key for key, _ in ordered_keys}

    for field, label in ordered_keys:
        value = answers.get(field)
        formatted = _format_profile_value(value)
        if formatted:
            sections.append(f"{label}: {formatted}")

    # Capture any extra onboarding answers that were not explicitly mapped.
    for key, value in answers.items():
        if key in used_keys:
            continue
        formatted = _format_profile_value(value)
        if formatted:
            sections.append(f"{key}: {formatted}")

    if fallback_question and fallback_question.strip():
        sections.append(f"ì¶”ê°€ ìš”ì²­: {fallback_question.strip()}")

    return "\n".join(sections).strip()


def _merge_tag_lists(existing: list[str], new_values: list[str]) -> list[str]:
    # ì „ê³µ íƒœê·¸ëŠ” ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆœì„œë¥¼ ë³´ì¡´í•˜ë©° í•©ì§‘í•© ì²˜ë¦¬
    merged = list(existing)
    for value in new_values:
        if value not in merged:
            merged.append(value)
    return merged


def _summarize_major_hits(hits, aggregated_scores, limit: int = 10):
    # Pinecone ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì „ê³µë³„ë¡œ ë¬¶ì–´ ìƒìœ„ doc_type/íƒœê·¸ ë“±ì„ ì •ë¦¬
    per_major: dict[str, dict] = {}

    # [ë²„ê·¸ ìˆ˜ì •] ì¤‘ë³µ ì œê±°
    # major_idê°€ ë‹¤ë¥´ë”ë¼ë„ major_nameì´ ê°™ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬
    seen_names = set()

    for hit in hits:
        if not hit.major_id:
            continue

        # ì´ë¦„ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬
        if hit.major_name in seen_names:
            # ì´ë¯¸ ê°™ì€ ì´ë¦„ì˜ ì „ê³µì´ ì¡´ì¬í•˜ë©´, ì ìˆ˜ê°€ ë” ë†’ì€ ê²½ìš° ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ ìŠ¤í‚µ
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ìŠ¤í‚µ (hitsê°€ ì ìˆ˜ìˆœ ì •ë ¬ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ë©´ ì²« ë²ˆì§¸ê°€ ë² ìŠ¤íŠ¸)
            # _summarize_major_hits í˜¸ì¶œ ì „ì— hitsê°€ ì •ë ¬ë˜ì–´ ìˆì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ
            # ë¡œì§ì„ ì¢€ ë” ì •êµí•˜ê²Œ: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” entryë¥¼ ì°¾ì•„ì„œ ë³‘í•©

            # ê¸°ì¡´ entry ì°¾ê¸° (ì´ë¦„ìœ¼ë¡œ)
            existing_id = None
            for mid, data in per_major.items():
                if data["major_name"] == hit.major_name:
                    existing_id = mid
                    break

            if existing_id:
                entry = per_major[existing_id]
            else:
                # Should not happen if seen_names logic is correct
                entry = per_major.setdefault(
                    hit.major_id,
                    {
                        "major_id": hit.major_id,
                        # ...
                    },
                )
        else:
            seen_names.add(hit.major_name)
            entry = per_major.setdefault(
                hit.major_id,
                {
                    "major_id": hit.major_id,
                    "major_name": hit.major_name,
                    "cluster": hit.metadata.get("cluster"),
                    "salary": hit.metadata.get("salary"),
                    "score": aggregated_scores.get(hit.major_id, 0.0),
                    "top_doc_types": {},
                    "sample_docs": [],
                    "relate_subject_tags": [],
                    "job_tags": [],
                    "summary": "",
                },
            )

        # ì ìˆ˜ ì—…ë°ì´íŠ¸ (Merge strategies)
        # ê°™ì€ ì´ë¦„ì˜ ë‹¤ë¥¸ IDê°€ ë“¤ì–´ì™”ì„ ë•Œ, ì ìˆ˜ê°€ ë” ë†’ë‹¤ë©´ score ì—…ë°ì´íŠ¸?
        # ì—¬ê¸°ì„œëŠ” aggregated_scoresê°€ ì´ë¯¸ IDë³„ë¡œ í•©ì‚°ë˜ì—ˆê¸° ë•Œë¬¸ì—
        # ì´ë¦„ì´ ê°™ì€ IDë“¤ì˜ ì ìˆ˜ë¥¼ í•©ì¹˜ê±°ë‚˜ Maxë¥¼ ì·¨í•´ì•¼ í•¨.
        # í˜„ì¬ëŠ” ë‹¨ìˆœ ìŠ¤í‚µ ë°©ì‹ì´ë‚˜ ë³‘í•© ë°©ì‹ì„ ì¨ì•¼ í•¨.
        # ì—¬ê¸°ì„œëŠ” "ë³‘í•©" ë°©ì‹ìœ¼ë¡œ êµ¬í˜„: íƒœê·¸ ë“±ì„ í•©ì¹¨.

        entry["top_doc_types"][hit.doc_type] = max(
            entry["top_doc_types"].get(hit.doc_type, 0.0),
            hit.score,
        )

        if len(entry["sample_docs"]) < 3:
            entry["sample_docs"].append(
                {
                    "doc_type": hit.doc_type,
                    "score": hit.score,
                    "text": hit.text,
                }
            )

        # summary doc_typeì¸ ê²½ìš° summary í•„ë“œì— ì €ì¥
        if hit.doc_type == "summary" and not entry["summary"]:
            entry["summary"] = hit.text

        entry["relate_subject_tags"] = _merge_tag_lists(
            entry["relate_subject_tags"],
            hit.metadata.get("relate_subject_tags", []) or [],
        )
        entry["job_tags"] = _merge_tag_lists(
            entry["job_tags"],
            hit.metadata.get("job_tags", []) or [],
        )

    for entry in per_major.values():
        entry["top_doc_types"] = sorted(
            entry["top_doc_types"].items(),
            key=lambda item: item[1],
            reverse=True,
        )

    ordered = sorted(
        per_major.values(),
        key=lambda item: item["score"],
        reverse=True,
    )
    return ordered[:limit]


def _normalize_majors_with_llm(raw_majors: list[str]) -> list[str]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì „ê³µëª…(ì¤„ì„ë§, ì˜¤íƒ€ ë“±)ì„ í‘œì¤€ ì „ê³µëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: ["ì»´ê³µ", "í™”ê³µ"] -> ["ì»´í“¨í„°ê³µí•™ê³¼", "í™”í•™ê³µí•™ê³¼"]
    """
    if not raw_majors:
        return []

    # ì…ë ¥ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ë¦¬ ë¹„ìš©ì´ í¬ë¯€ë¡œ ì œí•œ
    targets = raw_majors[:5]

    prompt = (
        "ì‚¬ìš©ìê°€ ì…ë ¥í•œ ëŒ€í•™ ì „ê³µëª…(ì¤„ì„ë§, ì˜¤íƒ€ í¬í•¨)ì„ ê°€ì¥ ì ì ˆí•œ 'í‘œì¤€ í•™ê³¼ëª…'ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n"
        "í•™ê³¼ëª…ì„ ëª…í™•íˆ íŒë‹¨í•˜ê¸° í˜ë“¤ê±°ë‚˜ ì˜¤íƒ€ê°€ ìˆëŠ”ê²½ìš°, ì‚¬ìš©ìì—ê²Œ ìì‹ ì´ ì¶”ë¡ í•œ 'í‘œì¤€ í•™ê³¼ëª…'ì´ ë§ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì ˆì°¨ë¥¼ ê±°ì³ì£¼ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ í•œêµ­ì–´ í•™ê³¼ëª…ë§Œ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ë‹¤ë¥¸ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        f"ì…ë ¥: {', '.join(targets)}\n"
        "ì¶œë ¥:"
    )

    try:
        response = llm.invoke(prompt)
        content = response.content.strip()

        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        normalized = [item.strip() for item in content.split(",") if item.strip()]
        print(f"ğŸ¤– LLM Normalized Majors: {targets} -> {normalized}")
        return normalized
    except Exception as e:
        print(f"âš ï¸ Failed to normalize majors with LLM: {e}")
        return targets  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜


def recommend_majors_node(state: MentorState) -> dict:
    """
    ì˜¨ë³´ë”© ë‹µë³€ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì í”„ë¡œí•„ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ì „ê³µì„ ìˆœìœ„ë³„ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤.
    ìš°ì„ ìˆœìœ„: preferred_majors ì •í™• ë§¤ì¹­ > ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    """
    onboarding_answers = state.get("onboarding_answers") or {}
    profile_text = _build_user_profile_text(onboarding_answers, state.get("question"))

    # ì˜¨ë³´ë”© ë‹µë³€ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
    if not profile_text:
        return {
            "user_profile_text": "",
            "recommended_majors": [],
            "major_search_hits": [],
            "major_scores": {},
        }

    # 1. ë²¡í„° ê²€ìƒ‰ (Vector Search)
    # ì˜¨ë³´ë”© í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ Pineconeì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì „ê³µ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    embeddings = get_embeddings()
    profile_embedding = embeddings.embed_query(profile_text)

    # Pineconeì—ì„œ ìƒìœ„ 50ê°œ ë¬¸ì„œ ê²€ìƒ‰
    hits = search_major_docs(profile_embedding, top_k=50)
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ì ìˆ˜ë¥¼ ì „ê³µë³„ë¡œ í•©ì‚°
    aggregated_scores = aggregate_major_scores(hits, MAJOR_DOC_WEIGHTS)

    recommended = _summarize_major_hits(hits, aggregated_scores)

    serialized_hits = [
        {
            "doc_id": hit.doc_id,
            "major_id": hit.major_id,
            "major_name": hit.major_name,
            "doc_type": hit.doc_type,
            "score": hit.score,
            "metadata": hit.metadata,
        }
        for hit in hits
    ]

    return {
        "user_profile_text": profile_text,
        "user_profile_embedding": profile_embedding,
        "major_search_hits": serialized_hits,
        "major_scores": aggregated_scores,
        "recommended_majors": recommended,
    }


# ==================== ReAct ìŠ¤íƒ€ì¼ ì—ì´ì „íŠ¸ ë…¸ë“œ ====================


def agent_node(state: MentorState) -> dict:
    """
    [ReAct íŒ¨í„´] LLMì´ ììœ¨ì ìœ¼ë¡œ tool í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •.
    """
    messages = state.get("messages", [])
    interests = state.get("interests")

    # system_messageëŠ” interests ìœ ë¬´ì™€ ìƒê´€ì—†ì´ í•­ìƒ ë§Œë“¤ì–´ë‘”ë‹¤.
    system_message = None
    if not messages or not any(isinstance(m, SystemMessage) for m in messages):
        interests_text = f"{interests}" if interests else "ì—†ìŒ"

        # âœ… f-string ë‚´ë¶€ JSON ì˜ˆì‹œëŠ” {{ }} ë¡œ ì´ìŠ¤ì¼€ì´í”„!
        system_message = SystemMessage(
            content=f"""
ë‹¹ì‹ ì€ í•™ìƒë“¤ì˜ ì „ê³µ ì„ íƒì„ ë•ëŠ” 'ëŒ€í•™ ì „ê³µ íƒìƒ‰ ë©˜í† 'ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[í•µì‹¬ ì›ì¹™]
1. **íˆ´ ê°€ì´ë“œ ì¤€ìˆ˜**: ê° íˆ´(get_major_career_info, get_university_admission_info ë“±)ì˜ ì„¤ëª…(docstring)ì— ëª…ì‹œëœ "ì‚¬ìš© ê·œì¹™"ê³¼ "ì œì•½ ì‚¬í•­"ì„ ì² ì €íˆ ë”°ë¥´ì„¸ìš”.
2. **í™˜ê° ë°©ì§€**: 
   - ì¼ë°˜ ì „ê³µ ì •ë³´(ì·¨ì—…ë¥ , ì—°ë´‰)ë¥¼ ì œê³µí•  ë•Œ, **ì ˆëŒ€ íŠ¹ì • ëŒ€í•™ì˜ ì •ë³´ì¸ ê²ƒì²˜ëŸ¼** ëŒ€í•™ëª…ì„ ë¶™ì—¬ì„œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.
   - ë°˜ë“œì‹œ "OOëŒ€í•™ì˜ êµ¬ì²´ì  ì •ë³´ëŠ” ì—†ì§€ë§Œ, ì¼ë°˜ì ì¸ OOí•™ê³¼ì˜ ì •ë³´ëŠ”..." í˜•íƒœë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
3. **ê·¼ê±° ê¸°ë°˜ ë‹µë³€**: ë°˜ë“œì‹œ íˆ´ í˜¸ì¶œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”. ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì—†ë‹¤ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”.
4. **ì¶œì²˜ ëª…ì‹œ í•„ìˆ˜**: 
   - `get_major_career_info`ë¥¼ í†µí•´ ì–»ì€ ì •ë³´(ì·¨ì—…ë¥ , ì—°ë´‰, ì§„ë¡œ ë“±)ëŠ” **ë°˜ë“œì‹œ 'ì»¤ë¦¬ì–´ë„·' ê¸°ë°˜ì„**ì„ ë°í˜€ì•¼ í•©ë‹ˆë‹¤.
   - **ì ˆëŒ€** "í•œì–‘ëŒ€í•™êµì˜ ì·¨ì—…ë¥ ì€..." ì´ë¼ê³  ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹  "í•œì–‘ëŒ€í•™êµì˜ ê³µì‹ ì·¨ì—…ë¥  ìë£ŒëŠ” í™•ì¸ë˜ì§€ ì•Šì•˜ìœ¼ë‚˜, ì»¤ë¦¬ì–´ë„·ì˜ ì¼ë°˜ì ì¸ ì»´í“¨í„°ê³µí•™ê³¼ ì·¨ì—…ë¥ ì€..." ì´ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
5. **ìœ ì‚¬ ì „ê³µ í—ˆìš©**: íˆ´ ê²€ìƒ‰ ê²°ê³¼ì— ì‚¬ìš©ìê°€ queryí•œ í•™ê³¼ëª…ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” í•™ê³¼ê°€ ìˆë‹¤ë©´, ê²€ìƒ‰ëœ í•™ê³¼ë¥¼ ì œì‹œí•˜ì„¸ìš”.
6. **ìº í¼ìŠ¤ êµ¬ë¶„**: 'ë³¸êµ'ì™€ 'ë¶„êµ(ERICA, ì„¸ì¢…, ê¸€ë¡œì»¬ ë“±)'ëŠ” ì„œë¡œ ë‹¤ë¥¸ ëŒ€í•™ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ëª…í™•íˆ êµ¬ë¶„í•´ì„œ ë‹µë³€í•˜ì„¸ìš”. (ì˜ˆ: "í•œì–‘ëŒ€í•™êµëŠ” ì»´í“¨í„°ì†Œí”„íŠ¸ì›¨ì–´í•™ë¶€, í•œì–‘ëŒ€í•™êµ ERICAëŠ” ì»´í“¨í„°í•™ë¶€ê°€ ê°œì„¤ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

[ì¶œë ¥ ì œì–´]
- **[ì¤‘ìš”] `get_major_career_info` í˜¸ì¶œ ì‹œ ìµœì í™”**: ì‚¬ìš©ìê°€ íŠ¹ì • ì •ë³´(ì˜ˆ: ì·¨ì—…ë¥ , ì§„ë¡œ, ë°°ìš°ëŠ” ê³¼ëª© ë“±)ë§Œ ë¬¼ì–´ë³´ëŠ” ê²½ìš°, `specific_field` íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë§Œ ìš”ì²­í•˜ì„¸ìš”. (ì˜ˆ: `specific_field='stats'`)
- ì‚¬ìš©ìê°€ ìš”ì²­í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ê³¼ë„í•˜ê²Œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ì§ˆë¬¸ì— í•„ìš”í•œ í•µì‹¬ ë‹µë³€ë§Œ ì œê³µí•˜ì„¸ìš”.
- ì¹œì ˆí•˜ê³  êµ¬ì¡°í™”ëœ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.
- **[ì¤‘ìš”]** ì‚¬ìš©ìê°€ ì²˜ìŒ ì¸ì‚¬ë¥¼ í•˜ê±°ë‚˜, ë¬´ì—‡ì„ í•´ì•¼ í• ì§€ ë¬¼ì–´ë³¼ ë•ŒëŠ” ë°˜ë“œì‹œ **"ì¶”ì²œ ì‹œì‘"** ê¸°ëŠ¥ì„ í†µí•´ ë§ì¶¤í˜• ì „ê³µ ì¶”ì²œì„ ë°›ì„ ìˆ˜ ìˆìŒì„ ì•ˆë‚´í•˜ì„¸ìš”. (ì˜ˆ: "ì €ì™€ í•¨ê»˜ ë‚˜ì—ê²Œ ë”± ë§ëŠ” ì „ê³µì„ ì°¾ì•„ë³¼ê¹Œìš”? 'ì¶”ì²œ ì‹œì‘'ì´ë¼ê³  ë§ì”€í•´ ì£¼ì„¸ìš”!")
- **[ì˜ˆì™¸ ì²˜ë¦¬]** ë§Œì•½ ì‚¬ìš©ìê°€ "ì¶”ì²œ ì‹œì‘"ì´ë¼ê³  ë§í–ˆëŠ”ë° ì´ ë©”ì‹œì§€ë¥¼ ë°›ì•˜ë‹¤ë©´(í”„ë¡ íŠ¸ì—”ë“œ íŠ¸ë¦¬ê±° ì‹¤íŒ¨), "í•™ê³¼ ëª©ë¡"ì„ ë‚˜ì—´í•˜ì§€ ë§ê³ , **"ì¶”ì²œ ê¸°ëŠ¥ì„ ì‹œì‘í•˜ë ¤ë©´ 'ì¶”ì²œ ì‹œì‘'ì„ ì •í™•íˆ ì…ë ¥í•´ ì£¼ì„¸ìš”."** ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”. ì ˆëŒ€ `list_departments` íˆ´ì„ í˜¸ì¶œí•˜ì—¬ ì¼ë°˜ í•™ê³¼ ëª©ë¡ì„ ë³´ì—¬ì£¼ì§€ ë§ˆì„¸ìš”.
                                       
í•™ìƒ ê´€ì‹¬ì‚¬: {interests_text}
"""
        )

    if system_message:
        messages = [system_message] + messages

    response = llm_with_tools.invoke(messages)

    # [MODIFICIATION] Removed internal retry loop to prevent token duplication in stream.
    # The prompt should be sufficient to encourage tool usage.
    # If the LLM responds without tools for greetings, it is acceptable.

    # 4. LLMì˜ ì‘ë‹µ(response)ì„ messagesì— ì¶”ê°€í•˜ì—¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    #    â†’ should_continueê°€ tool_calls ìœ ë¬´ë¥¼ í™•ì¸í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
    return {"messages": [response]}


def should_continue(state: MentorState) -> str:
    """
    [ReAct íŒ¨í„´ ë¼ìš°íŒ…] tool_calls ìˆìœ¼ë©´ tools ë…¸ë“œë¡œ, ì—†ìœ¼ë©´ ì¢…ë£Œ.
    """
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None

    if last_message and getattr(last_message, "tool_calls", None):
        return "tools"
    return "end"
