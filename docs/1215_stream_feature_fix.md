# Tool Calling 스트리밍 지연 해결

## 개요
Tool Calling을 포함한 멘토링 대화 시, 답변이 실시간으로 스트리밍되지 않고 툴 실행이 완료된 후 한꺼번에 출력되는 문제를 해결했습니다.

## 문제 원인
기존 `run_mentor_stream` 함수는 LangGraph의 `stream_mode="updates"`를 사용했습니다. 이 모드는 노드의 실행이 **완료된 후**에만 상태 업데이트를 반환하므로, LLM이 텍스트를 생성하는 도중(토큰 단위)의 출력은 클라이언트로 전달되지 않았습니다. Tool을 사용하는 경우 `agent` 노드 → `tools` 노드 → `agent` 노드 순서로 실행되는데, 각 노드가 끝날 때까지 대기 시간이 발생하여 사용자 경험이 저하되었습니다.

## 해결 방법
### 1. LangGraph Streaming Mode 변경
`stream_mode`를 `["messages", "updates"]` 복합 모드로 변경하여 다음 두 가지 데이터를 모두 수신하도록 했습니다.
- **messages**: LLM이 생성하는 토큰 단위의 메시지 청크 (실시간 텍스트 출력용)
- **updates**: 노드 실행 완료 후의 상태 정보 (Tool 호출 여부 확인 및 최종 DB 저장용)

### 2. Backend Logic 개선 (`views.py`)
- `messages` 스트림에서 `delta` 타입의 데이터를 추출하여 프론트엔드로 즉시 전송합니다.
- `updates` 스트림에서는 `agent` 노드가 툴을 호출했는지 확인하여 `status` 메시지("Tool: ...")를 전송합니다.
- DB 저장을 위한 전체 답변 내용은 `updates` 스트림에서 확보합니다.

### 3. Frontend Logic 개선 (`chat.js`)
- 서버로부터 `type: 'delta'` 메시지를 받으면, 기존 말풍선 내용에 텍스트를 이어 붙이는(append) 로직을 추가했습니다.
- `marked.parse`를 호출하여 마크다운 렌더링을 실시간으로 갱신합니다.

## 결과
- 이제 툴을 사용하는 복잡한 답변 과정에서도 사용자는 텍스트가 생성되는 과정을 실시간으로 볼 수 있습니다.
- "생각 중..."과 같은 긴 대기 시간이 줄어들고 반응성이 향상되었습니다.
